---
# Smart Librarian Export (v2.0)
- Page Number: 10
- Timestamp: 2025-12-14T14:59:18.950805+02:00
- Source URL: https://docs.pydantic.dev/latest/api/type_adapter
- Page Title: TypeAdapter - Pydantic Validation
- Semantic Filename: typeadapter_pydantic_validation.md
- Ollama Model: llama3.2:3b
- Export Mode: Smart Deep Crawl
- Statistics:
  - Status: Success
  - Content Length: 48,861 characters
---

# TypeAdapter - Pydantic Validation

[ Skip to content ](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter)
What's new â€” we've launched [Pydantic Logfire](https://pydantic.dev/articles/logfire-announcement) ![ðŸ”¥](https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.0.3/assets/svg/1f525.svg) to help you monitor and understand your [Pydantic validations.](https://logfire.pydantic.dev/docs/integrations/pydantic/)
# TypeAdapter
Bases: `T]`
Usage Documentation
[`TypeAdapter`](https://docs.pydantic.dev/latest/concepts/type_adapter/)
Type adapters provide a flexible way to perform validation and serialization based on a Python type.
A `TypeAdapter` instance exposes some of the functionality from `BaseModel` instance methods for types that do not have such methods (such as dataclasses, primitive types, and more).
**Note:** `TypeAdapter` instances are not types, and cannot be used as type annotations for fields.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`type` |  |  The type associated with the `TypeAdapter`. |  _required_  
`config` |  `ConfigDict[](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ConfigDict) | None` |  Configuration for the `TypeAdapter`, should be a dictionary conforming to [`ConfigDict`](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ConfigDict). Note You cannot provide a configuration when instantiating a `TypeAdapter` if the type you're using has its own config that cannot be overridden (ex: `BaseModel`, `TypedDict`, and `dataclass`). A [`type-adapter-config-unused`](https://docs.pydantic.dev/latest/errors/usage_errors/#type-adapter-config-unused) error will be raised in this case. |  `None`  
`_parent_depth` |  |  Depth at which to search for the `TypeAdapter` was instantiated. Note This parameter is named with an underscore to suggest its private nature and discourage use. It may be deprecated in a minor version, so we only recommend using it if you're comfortable with potential change in behavior/support. It's default value is 2 because internally, the `TypeAdapter` class makes another call to fetch the frame. |  `2`  
`module` |  |  The module that passes to plugin if provided. |  `None`  
Attributes:
Name | Type | Description  
---|---|---  
`core_schema` |  `CoreSchema` |  The core schema for the type.  
`validator` |  `SchemaValidator[](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.SchemaValidator) | PluggableSchemaValidator` |  The schema validator for the type.  
`serializer` |  `SchemaSerializer[](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.SchemaSerializer)` |  The schema serializer for the type.  
`pydantic_complete` |  |  Whether the core schema for the type is successfully built.  
Compatibility with `mypy`
Depending on the type used, `mypy` might raise an error when instantiating a `TypeAdapter`. As a workaround, you can explicitly annotate your variable:
```
fromtypingimport Union

frompydanticimport TypeAdapter

ta: TypeAdapter[Union[str, int]] = TypeAdapter(Union[str, int])  # type: ignore[arg-type]

```

Namespace management nuances and implementation details
Here, we collect some notes on namespace management, and subtle differences from `BaseModel`:
`BaseModel` uses its own `__module__` to find out where it was defined and then looks for symbols to resolve forward references in those globals. On the other hand, `TypeAdapter` can be initialized with arbitrary objects, which may not be types and thus do not have a `__module__` available. So instead we look at the globals in our parent stack frame.
It is expected that the `ns_resolver` passed to this function will have the correct namespace for the type we're adapting. See the source code for `TypeAdapter.__init__` and `TypeAdapter.rebuild` for various ways to construct this namespace.
This works for the case where this function is called in a module that has the target of forward references in its scope, but does not always work for more complex cases.
For example, take the following:
a.py```
IntList = list[int]
OuterDict = dict[str, 'IntList']

```

b.py```
fromaimport OuterDict

frompydanticimport TypeAdapter

IntList = int  # replaces the symbol the forward reference is looking for
v = TypeAdapter(OuterDict)
v({'x': 1})  # should fail but doesn't

```

If `OuterDict` were a `BaseModel`, this would work because it would resolve the forward reference within the `a.py` namespace. But `TypeAdapter(OuterDict)` can't determine what module `OuterDict` came from.
In other words, the assumption that _all_ forward references exist in the module we are being called from is not technically always true. Although most of the time it is and it works fine for recursive models and such, `BaseModel`'s behavior isn't perfect either and _can_ break in similar ways, so there is no right or wrong between the two.
But at the very least this behavior is _subtly_ different from `BaseModel`'s.
Source code in `pydantic/type_adapter.py`
```
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
```
| ```
def__init__(
    self,
    type: Any,
    *,
    config: ConfigDict | None = None,
    _parent_depth: int = 2,
    module: str | None = None,
) -> None:
    if _type_has_config(type) and config is not None:
        raise PydanticUserError(
            'Cannot use `config` when the type is a BaseModel, dataclass or TypedDict.'
            ' These types can have their own config and setting the config via the `config`'
            ' parameter to TypeAdapter will not override it, thus the `config` you passed to'
            ' TypeAdapter becomes meaningless, which is probably not what you want.',
            code='type-adapter-config-unused',
        )

    self._type = type
    self._config = config
    self._parent_depth = _parent_depth
    self.pydantic_complete = False

    parent_frame = self._fetch_parent_frame()
    if isinstance(type, types.FunctionType):
        # Special case functions, which are *not* pushed to the `NsResolver` stack and without this special case
        # would only have access to the parent namespace where the `TypeAdapter` was instantiated (if the function is defined
        # in another module, we need to look at that module's globals).
        if parent_frame is not None:
            # `f_locals` is the namespace where the type adapter was instantiated (~ to `f_globals` if at the module level):
            parent_ns = parent_frame.f_locals
        else:  # pragma: no cover
            parent_ns = None
        globalns, localns = _namespace_utils.ns_for_function(
            type,
            parent_namespace=parent_ns,
        )
        parent_namespace = None
    else:
        if parent_frame is not None:
            globalns = parent_frame.f_globals
            # Do not provide a local ns if the type adapter happens to be instantiated at the module level:
            localns = parent_frame.f_locals if parent_frame.f_locals is not globalns else {}
        else:  # pragma: no cover
            globalns = {}
            localns = {}
        parent_namespace = localns

    self._module_name = module or cast(str, globalns.get('__name__', ''))
    self._init_core_attrs(
        ns_resolver=_namespace_utils.NsResolver(
            namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),
            parent_namespace=parent_namespace,
        ),
        force=False,
    )

```
  
---|---  
##  rebuild [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.rebuild)
```
rebuild(
    *,
    force: = False,
    raise_errors: = True,
    _parent_namespace_depth: = 2,
    _types_namespace: MappingNamespace | None = None
) -> | None

```

Try to rebuild the pydantic-core schema for the adapter's type.
This may be necessary when one of the annotations is a ForwardRef which could not be resolved during the initial attempt to build the schema, and automatic rebuilding fails.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`force` |  |  Whether to force the rebuilding of the type adapter's schema, defaults to `False`. |  `False`  
`raise_errors` |  |  Whether to raise errors, defaults to `True`. |  `True`  
`_parent_namespace_depth` |  |  Depth at which to search for the  |  `2`  
`_types_namespace` |  `MappingNamespace | None` |  An explicit types namespace to use, instead of using the local namespace from the parent frame. Defaults to `None`. |  `None`  
Returns:
Type | Description  
---|---  
|  Returns `None` if the schema is already "complete" and rebuilding was not required.  
|  If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.  
Source code in `pydantic/type_adapter.py`
```
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
```
| ```
defrebuild(
    self,
    *,
    force: bool = False,
    raise_errors: bool = True,
    _parent_namespace_depth: int = 2,
    _types_namespace: _namespace_utils.MappingNamespace | None = None,
) -> bool | None:
"""Try to rebuild the pydantic-core schema for the adapter's type.

    This may be necessary when one of the annotations is a ForwardRef which could not be resolved during
    the initial attempt to build the schema, and automatic rebuilding fails.

    Args:
        force: Whether to force the rebuilding of the type adapter's schema, defaults to `False`.
        raise_errors: Whether to raise errors, defaults to `True`.
        _parent_namespace_depth: Depth at which to search for the [parent frame][frame-objects]. This
            frame is used when resolving forward annotations during schema rebuilding, by looking for
            the locals of this frame. Defaults to 2, which will result in the frame where the method
            was called.
        _types_namespace: An explicit types namespace to use, instead of using the local namespace
            from the parent frame. Defaults to `None`.

    Returns:
        Returns `None` if the schema is already "complete" and rebuilding was not required.
        If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.
    """
    if not force and self.pydantic_complete:
        return None

    if _types_namespace is not None:
        rebuild_ns = _types_namespace
    elif _parent_namespace_depth > 0:
        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}
    else:
        rebuild_ns = {}

    # we have to manually fetch globals here because there's no type on the stack of the NsResolver
    # and so we skip the globalns = get_module_ns_of(typ) call that would normally happen
    globalns = sys._getframe(max(_parent_namespace_depth - 1, 1)).f_globals
    ns_resolver = _namespace_utils.NsResolver(
        namespaces_tuple=_namespace_utils.NamespacesTuple(locals=rebuild_ns, globals=globalns),
        parent_namespace=rebuild_ns,
    )
    return self._init_core_attrs(ns_resolver=ns_resolver, force=True, raise_errors=raise_errors)

```
  
---|---  
##  validate_python [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_python)
```
validate_python(
    object: ,
    /,
    *,
    strict: | None = None,
    extra: ExtraValues[](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ExtraValues) | None = None,
    from_attributes: | None = None,
    context: | None = None,
    experimental_allow_partial: (
        | ["off", "on", "trailing-strings"]
    ) = False,
    by_alias: | None = None,
    by_name: | None = None,
) -> T

```

Validate a Python object against the model.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`object` |  |  The Python object to validate against the model. |  _required_  
`strict` |  |  Whether to strictly check types. |  `None`  
`extra` |  `ExtraValues[](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ExtraValues) | None` |  Whether to ignore, allow, or forbid extra data during model validation. See the [`extra` configuration value](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ConfigDict.extra) for details. |  `None`  
`from_attributes` |  |  Whether to extract data from object attributes. |  `None`  
`context` |  |  Additional context to pass to the validator. |  `None`  
`experimental_allow_partial` |  |  **Experimental** whether to enable [partial validation](https://docs.pydantic.dev/latest/concepts/experimental/#partial-validation), e.g. to process streams. * False / 'off': Default behavior, no partial validation. * True / 'on': Enable partial validation. * 'trailing-strings': Enable partial validation and allow trailing strings in the input. |  `False`  
`by_alias` |  |  Whether to use the field's alias when validating against the provided input data. |  `None`  
`by_name` |  |  Whether to use the field's name when validating against the provided input data. |  `None`  
Note
When using `TypeAdapter` with a Pydantic `dataclass`, the use of the `from_attributes` argument is not supported.
Returns:
Type | Description  
---|---  
`T` |  The validated object.  
Source code in `pydantic/type_adapter.py`
```
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
```
| ```
defvalidate_python(
    self,
    object: Any,
    /,
    *,
    strict: bool | None = None,
    extra: ExtraValues | None = None,
    from_attributes: bool | None = None,
    context: Any | None = None,
    experimental_allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False,
    by_alias: bool | None = None,
    by_name: bool | None = None,
) -> T:
"""Validate a Python object against the model.

    Args:
        object: The Python object to validate against the model.
        strict: Whether to strictly check types.
        extra: Whether to ignore, allow, or forbid extra data during model validation.
            See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.
        from_attributes: Whether to extract data from object attributes.
        context: Additional context to pass to the validator.
        experimental_allow_partial: **Experimental** whether to enable
            [partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.
            * False / 'off': Default behavior, no partial validation.
            * True / 'on': Enable partial validation.
            * 'trailing-strings': Enable partial validation and allow trailing strings in the input.
        by_alias: Whether to use the field's alias when validating against the provided input data.
        by_name: Whether to use the field's name when validating against the provided input data.

    !!! note
        When using `TypeAdapter` with a Pydantic `dataclass`, the use of the `from_attributes`
        argument is not supported.

    Returns:
        The validated object.
    """
    if by_alias is False and by_name is not True:
        raise PydanticUserError(
            'At least one of `by_alias` or `by_name` must be set to True.',
            code='validate-by-alias-and-name-false',
        )

    return self.validator.validate_python(
        object,
        strict=strict,
        extra=extra,
        from_attributes=from_attributes,
        context=context,
        allow_partial=experimental_allow_partial,
        by_alias=by_alias,
        by_name=by_name,
    )

```
  
---|---  
##  validate_json [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_json)
```
validate_json(
    data: | | ,
    /,
    *,
    strict: | None = None,
    extra: ExtraValues[](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ExtraValues) | None = None,
    context: | None = None,
    experimental_allow_partial: (
        | ["off", "on", "trailing-strings"]
    ) = False,
    by_alias: | None = None,
    by_name: | None = None,
) -> T

```

Usage Documentation
[JSON Parsing](https://docs.pydantic.dev/latest/concepts/json/#json-parsing)
Validate a JSON string or bytes against the model.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`data` |  |  The JSON data to validate against the model. |  _required_  
`strict` |  |  Whether to strictly check types. |  `None`  
`extra` |  `ExtraValues[](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ExtraValues) | None` |  Whether to ignore, allow, or forbid extra data during model validation. See the [`extra` configuration value](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ConfigDict.extra) for details. |  `None`  
`context` |  |  Additional context to use during validation. |  `None`  
`experimental_allow_partial` |  |  **Experimental** whether to enable [partial validation](https://docs.pydantic.dev/latest/concepts/experimental/#partial-validation), e.g. to process streams. * False / 'off': Default behavior, no partial validation. * True / 'on': Enable partial validation. * 'trailing-strings': Enable partial validation and allow trailing strings in the input. |  `False`  
`by_alias` |  |  Whether to use the field's alias when validating against the provided input data. |  `None`  
`by_name` |  |  Whether to use the field's name when validating against the provided input data. |  `None`  
Returns:
Type | Description  
---|---  
`T` |  The validated object.  
Source code in `pydantic/type_adapter.py`
```
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
```
| ```
defvalidate_json(
    self,
    data: str | bytes | bytearray,
    /,
    *,
    strict: bool | None = None,
    extra: ExtraValues | None = None,
    context: Any | None = None,
    experimental_allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False,
    by_alias: bool | None = None,
    by_name: bool | None = None,
) -> T:
"""!!! abstract "Usage Documentation"
        [JSON Parsing](../concepts/json.md#json-parsing)

    Validate a JSON string or bytes against the model.

    Args:
        data: The JSON data to validate against the model.
        strict: Whether to strictly check types.
        extra: Whether to ignore, allow, or forbid extra data during model validation.
            See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.
        context: Additional context to use during validation.
        experimental_allow_partial: **Experimental** whether to enable
            [partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.
            * False / 'off': Default behavior, no partial validation.
            * True / 'on': Enable partial validation.
            * 'trailing-strings': Enable partial validation and allow trailing strings in the input.
        by_alias: Whether to use the field's alias when validating against the provided input data.
        by_name: Whether to use the field's name when validating against the provided input data.

    Returns:
        The validated object.
    """
    if by_alias is False and by_name is not True:
        raise PydanticUserError(
            'At least one of `by_alias` or `by_name` must be set to True.',
            code='validate-by-alias-and-name-false',
        )

    return self.validator.validate_json(
        data,
        strict=strict,
        extra=extra,
        context=context,
        allow_partial=experimental_allow_partial,
        by_alias=by_alias,
        by_name=by_name,
    )

```
  
---|---  
##  validate_strings [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_strings)
```
validate_strings(
    obj: ,
    /,
    *,
    strict: | None = None,
    extra: ExtraValues[](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ExtraValues) | None = None,
    context: | None = None,
    experimental_allow_partial: (
        | ["off", "on", "trailing-strings"]
    ) = False,
    by_alias: | None = None,
    by_name: | None = None,
) -> T

```

Validate object contains string data against the model.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`obj` |  |  The object contains string data to validate. |  _required_  
`strict` |  |  Whether to strictly check types. |  `None`  
`extra` |  `ExtraValues[](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ExtraValues) | None` |  Whether to ignore, allow, or forbid extra data during model validation. See the [`extra` configuration value](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ConfigDict.extra) for details. |  `None`  
`context` |  |  Additional context to use during validation. |  `None`  
`experimental_allow_partial` |  |  **Experimental** whether to enable [partial validation](https://docs.pydantic.dev/latest/concepts/experimental/#partial-validation), e.g. to process streams. * False / 'off': Default behavior, no partial validation. * True / 'on': Enable partial validation. * 'trailing-strings': Enable partial validation and allow trailing strings in the input. |  `False`  
`by_alias` |  |  Whether to use the field's alias when validating against the provided input data. |  `None`  
`by_name` |  |  Whether to use the field's name when validating against the provided input data. |  `None`  
Returns:
Type | Description  
---|---  
`T` |  The validated object.  
Source code in `pydantic/type_adapter.py`
```
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
```
| ```
defvalidate_strings(
    self,
    obj: Any,
    /,
    *,
    strict: bool | None = None,
    extra: ExtraValues | None = None,
    context: Any | None = None,
    experimental_allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False,
    by_alias: bool | None = None,
    by_name: bool | None = None,
) -> T:
"""Validate object contains string data against the model.

    Args:
        obj: The object contains string data to validate.
        strict: Whether to strictly check types.
        extra: Whether to ignore, allow, or forbid extra data during model validation.
            See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.
        context: Additional context to use during validation.
        experimental_allow_partial: **Experimental** whether to enable
            [partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.
            * False / 'off': Default behavior, no partial validation.
            * True / 'on': Enable partial validation.
            * 'trailing-strings': Enable partial validation and allow trailing strings in the input.
        by_alias: Whether to use the field's alias when validating against the provided input data.
        by_name: Whether to use the field's name when validating against the provided input data.

    Returns:
        The validated object.
    """
    if by_alias is False and by_name is not True:
        raise PydanticUserError(
            'At least one of `by_alias` or `by_name` must be set to True.',
            code='validate-by-alias-and-name-false',
        )

    return self.validator.validate_strings(
        obj,
        strict=strict,
        extra=extra,
        context=context,
        allow_partial=experimental_allow_partial,
        by_alias=by_alias,
        by_name=by_name,
    )

```
  
---|---  
##  get_default_value [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.get_default_value)
```
get_default_value(
    *,
    strict: | None = None,
    context: | None = None
) -> Some[](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.Some)[T] | None

```

Get the default value for the wrapped type.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`strict` |  |  Whether to strictly check types. |  `None`  
`context` |  |  Additional context to pass to the validator. |  `None`  
Returns:
Type | Description  
---|---  
`Some[](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.Some)[T] | None` |  The default value wrapped in a `Some` if there is one or None if not.  
Source code in `pydantic/type_adapter.py`
```
549
550
551
552
553
554
555
556
557
558
559
```
| ```
defget_default_value(self, *, strict: bool | None = None, context: Any | None = None) -> Some[T] | None:
"""Get the default value for the wrapped type.

    Args:
        strict: Whether to strictly check types.
        context: Additional context to pass to the validator.

    Returns:
        The default value wrapped in a `Some` if there is one or None if not.
    """
    return self.validator.get_default_value(strict=strict, context=context)

```
  
---|---  
##  dump_python [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.dump_python)
```
dump_python(
    instance: T,
    /,
    *,
    mode: ["json", "python"] = "python",
    include: IncEx | None = None,
    exclude: IncEx | None = None,
    by_alias: | None = None,
    exclude_unset: = False,
    exclude_defaults: = False,
    exclude_none: = False,
    exclude_computed_fields: = False,
    round_trip: = False,
    warnings: (
        | ["none", "warn", "error"]
    ) = True,
    fallback: [[], ] | None = None,
    serialize_as_any: = False,
    context: | None = None,
) -> 
```

Dump an instance of the adapted type to a Python object.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`instance` |  `T` |  The Python object to serialize. |  _required_  
`mode` |  |  The output format. |  `'python'`  
`include` |  `IncEx | None` |  Fields to include in the output. |  `None`  
`exclude` |  `IncEx | None` |  Fields to exclude from the output. |  `None`  
`by_alias` |  |  Whether to use alias names for field names. |  `None`  
`exclude_unset` |  |  Whether to exclude unset fields. |  `False`  
`exclude_defaults` |  |  Whether to exclude fields with default values. |  `False`  
`exclude_none` |  |  Whether to exclude fields with None values. |  `False`  
`exclude_computed_fields` |  |  Whether to exclude computed fields. While this can be useful for round-tripping, it is usually recommended to use the dedicated `round_trip` parameter instead. |  `False`  
`round_trip` |  |  Whether to output the serialized data in a way that is compatible with deserialization. |  `False`  
`warnings` |  |  How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors, "error" raises a [`PydanticSerializationError`](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.PydanticSerializationError). |  `True`  
`fallback` |  |  A function to call when an unknown value is encountered. If not provided, a [`PydanticSerializationError`](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.PydanticSerializationError) error is raised. |  `None`  
`serialize_as_any` |  |  Whether to serialize fields with duck-typing serialization behavior. |  `False`  
`context` |  |  Additional context to pass to the serializer. |  `None`  
Returns:
Type | Description  
---|---  
|  The serialized object.  
Source code in `pydantic/type_adapter.py`
```
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
```
| ```
defdump_python(
    self,
    instance: T,
    /,
    *,
    mode: Literal['json', 'python'] = 'python',
    include: IncEx | None = None,
    exclude: IncEx | None = None,
    by_alias: bool | None = None,
    exclude_unset: bool = False,
    exclude_defaults: bool = False,
    exclude_none: bool = False,
    exclude_computed_fields: bool = False,
    round_trip: bool = False,
    warnings: bool | Literal['none', 'warn', 'error'] = True,
    fallback: Callable[[Any], Any] | None = None,
    serialize_as_any: bool = False,
    context: Any | None = None,
) -> Any:
"""Dump an instance of the adapted type to a Python object.

    Args:
        instance: The Python object to serialize.
        mode: The output format.
        include: Fields to include in the output.
        exclude: Fields to exclude from the output.
        by_alias: Whether to use alias names for field names.
        exclude_unset: Whether to exclude unset fields.
        exclude_defaults: Whether to exclude fields with default values.
        exclude_none: Whether to exclude fields with None values.
        exclude_computed_fields: Whether to exclude computed fields.
            While this can be useful for round-tripping, it is usually recommended to use the dedicated
            `round_trip` parameter instead.
        round_trip: Whether to output the serialized data in a way that is compatible with deserialization.
        warnings: How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
            "error" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].
        fallback: A function to call when an unknown value is encountered. If not provided,
            a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.
        serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.
        context: Additional context to pass to the serializer.

    Returns:
        The serialized object.
    """
    return self.serializer.to_python(
        instance,
        mode=mode,
        by_alias=by_alias,
        include=include,
        exclude=exclude,
        exclude_unset=exclude_unset,
        exclude_defaults=exclude_defaults,
        exclude_none=exclude_none,
        exclude_computed_fields=exclude_computed_fields,
        round_trip=round_trip,
        warnings=warnings,
        fallback=fallback,
        serialize_as_any=serialize_as_any,
        context=context,
    )

```
  
---|---  
##  dump_json [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.dump_json)
```
dump_json(
    instance: T,
    /,
    *,
    indent: | None = None,
    ensure_ascii: = False,
    include: IncEx | None = None,
    exclude: IncEx | None = None,
    by_alias: | None = None,
    exclude_unset: = False,
    exclude_defaults: = False,
    exclude_none: = False,
    exclude_computed_fields: = False,
    round_trip: = False,
    warnings: (
        | ["none", "warn", "error"]
    ) = True,
    fallback: [[], ] | None = None,
    serialize_as_any: = False,
    context: | None = None,
) -> 
```

Usage Documentation
[JSON Serialization](https://docs.pydantic.dev/latest/concepts/json/#json-serialization)
Serialize an instance of the adapted type to JSON.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`instance` |  `T` |  The instance to be serialized. |  _required_  
`indent` |  |  Number of spaces for JSON indentation. |  `None`  
`ensure_ascii` |  |  If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped. If `False` (the default), these characters will be output as-is. |  `False`  
`include` |  `IncEx | None` |  Fields to include. |  `None`  
`exclude` |  `IncEx | None` |  Fields to exclude. |  `None`  
`by_alias` |  |  Whether to use alias names for field names. |  `None`  
`exclude_unset` |  |  Whether to exclude unset fields. |  `False`  
`exclude_defaults` |  |  Whether to exclude fields with default values. |  `False`  
`exclude_none` |  |  Whether to exclude fields with a value of `None`. |  `False`  
`exclude_computed_fields` |  |  Whether to exclude computed fields. While this can be useful for round-tripping, it is usually recommended to use the dedicated `round_trip` parameter instead. |  `False`  
`round_trip` |  |  Whether to serialize and deserialize the instance to ensure round-tripping. |  `False`  
`warnings` |  |  How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors, "error" raises a [`PydanticSerializationError`](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.PydanticSerializationError). |  `True`  
`fallback` |  |  A function to call when an unknown value is encountered. If not provided, a [`PydanticSerializationError`](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.PydanticSerializationError) error is raised. |  `None`  
`serialize_as_any` |  |  Whether to serialize fields with duck-typing serialization behavior. |  `False`  
`context` |  |  Additional context to pass to the serializer. |  `None`  
Returns:
Type | Description  
---|---  
|  The JSON representation of the given instance as bytes.  
Source code in `pydantic/type_adapter.py`
```
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
```
| ```
defdump_json(
    self,
    instance: T,
    /,
    *,
    indent: int | None = None,
    ensure_ascii: bool = False,
    include: IncEx | None = None,
    exclude: IncEx | None = None,
    by_alias: bool | None = None,
    exclude_unset: bool = False,
    exclude_defaults: bool = False,
    exclude_none: bool = False,
    exclude_computed_fields: bool = False,
    round_trip: bool = False,
    warnings: bool | Literal['none', 'warn', 'error'] = True,
    fallback: Callable[[Any], Any] | None = None,
    serialize_as_any: bool = False,
    context: Any | None = None,
) -> bytes:
"""!!! abstract "Usage Documentation"
        [JSON Serialization](../concepts/json.md#json-serialization)

    Serialize an instance of the adapted type to JSON.

    Args:
        instance: The instance to be serialized.
        indent: Number of spaces for JSON indentation.
        ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.
            If `False` (the default), these characters will be output as-is.
        include: Fields to include.
        exclude: Fields to exclude.
        by_alias: Whether to use alias names for field names.
        exclude_unset: Whether to exclude unset fields.
        exclude_defaults: Whether to exclude fields with default values.
        exclude_none: Whether to exclude fields with a value of `None`.
        exclude_computed_fields: Whether to exclude computed fields.
            While this can be useful for round-tripping, it is usually recommended to use the dedicated
            `round_trip` parameter instead.
        round_trip: Whether to serialize and deserialize the instance to ensure round-tripping.
        warnings: How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
            "error" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].
        fallback: A function to call when an unknown value is encountered. If not provided,
            a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.
        serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.
        context: Additional context to pass to the serializer.

    Returns:
        The JSON representation of the given instance as bytes.
    """
    return self.serializer.to_json(
        instance,
        indent=indent,
        ensure_ascii=ensure_ascii,
        include=include,
        exclude=exclude,
        by_alias=by_alias,
        exclude_unset=exclude_unset,
        exclude_defaults=exclude_defaults,
        exclude_none=exclude_none,
        exclude_computed_fields=exclude_computed_fields,
        round_trip=round_trip,
        warnings=warnings,
        fallback=fallback,
        serialize_as_any=serialize_as_any,
        context=context,
    )

```
  
---|---  
##  json_schema [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.json_schema)
```
json_schema(
    *,
    by_alias: = True,
    ref_template: = DEFAULT_REF_TEMPLATE[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.DEFAULT_REF_TEMPLATE),
    union_format: [
        "any_of", "primitive_type_array"
    ] = "any_of",
    schema_generator: [
        GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema)
    ] = GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema),
    mode: JsonSchemaMode[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode) = "validation"
) -> [, ]

```

Generate a JSON schema for the adapted type.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`by_alias` |  |  Whether to use alias names for field names. |  `True`  
`ref_template` |  |  The format string used for generating $ref strings. |  `DEFAULT_REF_TEMPLATE[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.DEFAULT_REF_TEMPLATE)`  
`union_format` |  |  The format to use when combining schemas from unions together. Can be one of:
  * `'any_of'`: Use the 
  * `'primitive_type_array'`: Use the `string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to `any_of`.

|  `'any_of'`  
`schema_generator` |  `GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema)]` |  To override the logic used to generate the JSON schema, as a subclass of `GenerateJsonSchema` with your desired modifications |  `GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema)`  
`mode` |  `JsonSchemaMode[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode)` |  The mode in which to generate the schema. |  `'validation'`  
`schema_generator` |  `GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema)]` |  The generator class used for creating the schema. |  `GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema)`  
`mode` |  `JsonSchemaMode[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode)` |  The mode to use for schema generation. |  `'validation'`  
Returns:
Type | Description  
---|---  
|  The JSON schema for the model as a dictionary.  
Source code in `pydantic/type_adapter.py`
```
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
```
| ```
defjson_schema(
    self,
    *,
    by_alias: bool = True,
    ref_template: str = DEFAULT_REF_TEMPLATE,
    union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',
    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,
    mode: JsonSchemaMode = 'validation',
) -> dict[str, Any]:
"""Generate a JSON schema for the adapted type.

    Args:
        by_alias: Whether to use alias names for field names.
        ref_template: The format string used for generating $ref strings.
        union_format: The format to use when combining schemas from unions together. Can be one of:

            - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)
            keyword to combine schemas (the default).
            - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)
            keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive
            type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to
            `any_of`.
        schema_generator: To override the logic used to generate the JSON schema, as a subclass of
            `GenerateJsonSchema` with your desired modifications
        mode: The mode in which to generate the schema.
        schema_generator: The generator class used for creating the schema.
        mode: The mode to use for schema generation.

    Returns:
        The JSON schema for the model as a dictionary.
    """
    schema_generator_instance = schema_generator(
        by_alias=by_alias, ref_template=ref_template, union_format=union_format
    )
    if isinstance(self.core_schema, _mock_val_ser.MockCoreSchema):
        self.core_schema.rebuild()
        assert not isinstance(self.core_schema, _mock_val_ser.MockCoreSchema), 'this is a bug! please report it'
    return schema_generator_instance.generate(self.core_schema, mode=mode)

```
  
---|---  
##  json_schemas `staticmethod` [Â¶](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.json_schemas)
```
json_schemas(
    inputs: [
        [
            JsonSchemaKeyT, JsonSchemaMode[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode), TypeAdapter[](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter)[]
        ]
    ],
    /,
    *,
    by_alias: = True,
    title: | None = None,
    description: | None = None,
    ref_template: = DEFAULT_REF_TEMPLATE[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.DEFAULT_REF_TEMPLATE),
    union_format: [
        "any_of", "primitive_type_array"
    ] = "any_of",
    schema_generator: [
        GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema)
    ] = GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema),
) -> [
    [
        [JsonSchemaKeyT, JsonSchemaMode[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode)],
        JsonSchemaValue[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaValue),
    ],
    JsonSchemaValue[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaValue),
]

```

Generate a JSON schema including definitions from multiple type adapters.
Parameters:
Name | Type | Description | Default  
---|---|---|---  
`inputs` |  `JsonSchemaKeyT, JsonSchemaMode[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode), TypeAdapter[](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter)[` |  Inputs to schema generation. The first two items will form the keys of the (first) output mapping; the type adapters will provide the core schemas that get converted into definitions in the output JSON schema. |  _required_  
`by_alias` |  |  Whether to use alias names. |  `True`  
`title` |  |  The title for the schema. |  `None`  
`description` |  |  The description for the schema. |  `None`  
`ref_template` |  |  The format string used for generating $ref strings. |  `DEFAULT_REF_TEMPLATE[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.DEFAULT_REF_TEMPLATE)`  
`union_format` |  |  The format to use when combining schemas from unions together. Can be one of:
  * `'any_of'`: Use the 
  * `'primitive_type_array'`: Use the `string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to `any_of`.

|  `'any_of'`  
`schema_generator` |  `GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema)]` |  The generator class used for creating the schema. |  `GenerateJsonSchema[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema)`  
Returns:
Type | Description  
---|---  
`JsonSchemaKeyT, JsonSchemaMode[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode)], JsonSchemaValue[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaValue)], JsonSchemaValue[](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.JsonSchemaValue)]` |  A tuple where:
  * The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have JsonRef references to definitions that are defined in the second returned element.)
  * The second element is a JSON schema containing all definitions referenced in the first returned element, along with the optional title and description keys.

  
Source code in `pydantic/type_adapter.py`
```
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
```
| ```
@staticmethod
defjson_schemas(
    inputs: Iterable[tuple[JsonSchemaKeyT, JsonSchemaMode, TypeAdapter[Any]]],
    /,
    *,
    by_alias: bool = True,
    title: str | None = None,
    description: str | None = None,
    ref_template: str = DEFAULT_REF_TEMPLATE,
    union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',
    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,
) -> tuple[dict[tuple[JsonSchemaKeyT, JsonSchemaMode], JsonSchemaValue], JsonSchemaValue]:
"""Generate a JSON schema including definitions from multiple type adapters.

    Args:
        inputs: Inputs to schema generation. The first two items will form the keys of the (first)
            output mapping; the type adapters will provide the core schemas that get converted into
            definitions in the output JSON schema.
        by_alias: Whether to use alias names.
        title: The title for the schema.
        description: The description for the schema.
        ref_template: The format string used for generating $ref strings.
        union_format: The format to use when combining schemas from unions together. Can be one of:

            - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)
            keyword to combine schemas (the default).
            - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)
            keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive
            type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to
            `any_of`.
        schema_generator: The generator class used for creating the schema.

    Returns:
        A tuple where:

            - The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and
                whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have
                JsonRef references to definitions that are defined in the second returned element.)
            - The second element is a JSON schema containing all definitions referenced in the first returned
                element, along with the optional title and description keys.

    """
    schema_generator_instance = schema_generator(
        by_alias=by_alias, ref_template=ref_template, union_format=union_format
    )

    inputs_ = []
    for key, mode, adapter in inputs:
        # This is the same pattern we follow for model json schemas - we attempt a core schema rebuild if we detect a mock
        if isinstance(adapter.core_schema, _mock_val_ser.MockCoreSchema):
            adapter.core_schema.rebuild()
            assert not isinstance(adapter.core_schema, _mock_val_ser.MockCoreSchema), (
                'this is a bug! please report it'
            )
        inputs_.append((key, mode, adapter.core_schema))

    json_schemas_map, definitions = schema_generator_instance.generate_definitions(inputs_)

    json_schema: dict[str, Any] = {}
    if definitions:
        json_schema['$defs'] = definitions
    if title:
        json_schema['title'] = title
    if description:
        json_schema['description'] = description

    return json_schemas_map, json_schema

```
  
---|---  
Was this page helpful? 
Thanks for your feedback! 
Thanks for your feedback! 
